# SHAIL v2 on Apple Silicon

In this version of SHAIL, we use **ollama* LLM model host and **open-webui** to host the LLMs.

## 1. Set up Ollama

[Ollama](https://ollama.com) gets you up and running quickly on large language models.
Just visit the site and download ollama to your local machine.
Copy the downloaded Olama app to the *Applications* folder in the Mac.
Now you can start playing with ollama locally.

You can read the open source code and details at <https://github.com/ollama/ollama>

### Simple commands for ollama

Here are a quick set of commands that are useful

- ollama help - shows the help for the ollama tool
- ollama list - lists the models in play
- ollama ps - look at what ollama processes are running
- ollama pull deepseek-r1 - gets the latest model for deepseek-r1 (default is 7B model)
- ollama run deepseek-r1 - runs the model and is accessible within the terminal in text mode

NOTE: ollama saves configurations inside *~/.ollama*.

Search and download models from [ollama models](https://ollama.com/search). 
Several open source models in various distillation forms are avaiable.

## 2. Configure and run with your system prompt

Ollama allows one to create multiple small tailored instances of LLMs to run.
Use a system prompt to configure the details and launch Ollama with the specific configuration.

For example, here is a way to create a LLM tailored to create meme based on inputs.
See the [Meme Creator Model File](../scripts/meme_maker.modelfile)

```sh

# create a custom meme model
ollama create meme_maker -f ../scripts/meme_maker.modelfile

# run the meme_maker model to ask questions
ollama run meme_maker

```

When I asked it to generate meme for *ollama is cool*, then it gives me back this response. *Ollama Pro: Because it's smarter than your average assistant! üòé*

## 3. Running Ollama as API server

It is very easy to turnaround and run the ollama in a headless mode for API access.

```sh

# start the ollama server
ollama serve

open http://localhost:11434 for accessing the server
```

Once you have the server running, you can acces this via API.
Here is an example for generating a meme for the Super Bowl victory by Eagles (2025).

```sh

# get meme for the super bowl victory by Eagles in 2025
curl -X POST http://localhost:11434/api/generate \
  -H 'Content-Type: application/json' \
  -d '{
  "model": "meme_maker",
  "prompt": "Superbowl victory by Eagles",
  "stream": false
}'
```

For this I get a detailed API response in JSON format

```json
{
    "model": "meme_maker",
    "created_at": "2025-02-10T03:27:27.34914Z",
    "response": "<think>\n ... all reasoning logic is here\n</think>\n\n\"Eagles Fly Super Bowl: People forget we exist because we're too busy winning!\"\n\nThis meme plays on the Eagles' famous song and humorously suggests their victory leads to public disinterest.",
    "done": true,
    "done_reason": "stop",
    "context": [ 131, 393, ... ],
    "total_duration": 10200287917,
    "load_duration": 33825209,
    "prompt_eval_count": 113,
    "prompt_eval_duration": 279000000,
    "eval_count": 275,
    "eval_duration": 9886000000
}
```

if the input request had *‚Äústream‚Äù: true*, the response is as stream in the following format

```json
...
{"model":"di_challenge","created_at":"2025-02-10T03:31:23.4312Z","response":"\u003cthink\u003e","done":false}
...
{"model":"di_challenge","created_at":"2025-02-10T03:31:23.7241Z","response":"\"Eagles", "done":false}
...
```

Once we are done using the service, we can stop the model using "stop" command

```sh
# Stop the Meme Maker
ollama stop meme_maker
```

The ollama server may keep running in the background at port 11434.
You can quit this process one of the following steps:

1. Find the ollama icon in the running icons list (at top right) and quit it
2. Force quit the processs
3. Find process ID and kill the process

## 4. Use with Open Web UI

[Open-WebUI](https://openwebui.com) is super awesome UI tool for running AI models.
This project comes with a state of the art tool for setting up various models, configuring users, etc. It provides a large variety of configuration knobs to host all types of models. The Open WebUI site also supports a variety of models. Plus it automatically works with Ollama hosted models.

### 4.1 Use docker container for Open-WebUI

To keep things isolated, there is the docker container way to host and run Open-WebUI.

```sh
# docker used with olama running outside the docker container 

docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main

```

Access and use the dockerized Open-WebUI at <http://localhost:3000>.
Set up main admin account
Now you can use the chat interface to play with the model.
As of Feb 9, 2025, it defaults to using the *DeepSeek-R1:14b* model.

NOTE: make sure to have docker installed on your machine. Get docker for Mac from <https://docs.docker.com/desktop/setup/install/mac-install/>

### 4.2 Install and use Python version of Open-WebUI

```sh
# install the tool
pip install open-webui
# resolve any conflicts from python packages

# start running the model
open-webui serve

# the server now runs at http://localhost:8080
```

Access the local [Open-WebUI site](http://localhost:8080).
Set up main admin account
Now you can use the chat interface to play with the model.
As of Feb 9, 2025, it defaults to using the *DeepSeek-R1:14b* model.

## References

There are a lot of add-ons for Ollama. Check them out. Here are some of interest to me.

- [Ollama repo](https://github.com/ollama/ollama)
- [OllamaKit](https://github.com/kevinhermawan/OllamaKit)
- [Ollama-Swift UI](https://github.com/kghandour/Ollama-SwiftUI)

- [Open-WebUI repo](https://github.com/open-webui/open-webui)
- [Open-WebU whitepaper](https://openwebui.com/assets/files/whitepaper.pdf)
